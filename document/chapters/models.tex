\chapter{Models of Neural Networks}
\label{sec:models}
%
Fundamentally, a neural network can be viewed as a directed computation graph, where nodes correspond to operations (e.g., addition, multiplication, activation functions) over tensors, and edges represent the flow of data between these operations. Inference is performed by applying these operations sequentially to one or more input tensors, propagating values through the graph, and producing one or more output tensors.

Many formats exist for modelling neural networks, each serving different use cases. 
Formats such as TensorFlow’s \texttt{.ckpt} or PyTorch’s \texttt{.pth} files are used within their own specific ecosystems, aimed at providing representations that allow the model to be trained. 
In contrast, interoperable formats aim to decouple model representation from a particular framework or hardware backend, enabling broader reuse and tool support. 
Notable examples include ONNX (Open Neural Network Exchange), NNEF (Neural Network Exchange Format), GGUF (GPT-Generated Unified Format) and Safetensors.

\section{ONNX}
\label{sec:onnx}

To formally reason about whether a query about a neural network is satisfiable, agreeing upon a standardised representation for neural network models is essential.
The \vnnlib{} standard uses ONNX as the standard format for representing neural network models, for the following reasons:
\begin{itemize}
\item \textbf{Community owned}: ONNX is a community-driven rather than a proprietary project. This reduces the risk of the discontinuation of the standard and ensures greater neutrality and transparency in its governance.
\item \textbf{Framework and Hardware Agnostic}: ONNX is designed to be independent of both training frameworks and deployment environments. This makes it well-suited to serve as a common exchange format across diverse toolchains and platforms.
\item \textbf{Widespread Adoption}: ONNX is widely supported by major frameworks, including PyTorch and TensorFlow, which provide tooling for exporting to and importing from the ONNX format. This facilitates model sharing and conversion.
\item \textbf{Rich and Extensible Operator Set}: ONNX includes a comprehensive set of standardised operators, enabling it to express a wide variety of neural network architectures. Its extensibility also allows new operators to be proposed and adopted by the community as needed.

\item \textbf{Versioning and Documentation}: The ONNX specification includes detailed documentation and operator definitions, and a robust versioning system. This makes it easier for tools to implement and interact with the format in a consistent manner.

\item \textbf{Existing Verifier Ecosystem}: Finally, ONNX is already well-integrated into the neural network verification research community. It has served as the standard format in the VNN-COMP competition, making it a natural fit for \vnnlib{} and aligning with existing verifier tooling.
\end{itemize}
We will now describe ONNX's syntax (i.e. the format of ONNX models) and ONNX's semantics (i.e. the computation that ONNX models represent).

\subsection{Syntax}
\label{sec:onnx_overview}

The ONNX format provides a standardised syntax for specifying the computations performed by a neural network.
We will now give a brief overview of the ONNX format. For a more detailed description suitable for implementators of neural network verifiers, the full ONNX specification should be consulted. 

\paragraph{Model structure} 

An ONNX network is serialised as a single binary file with the \texttt{.onnx} file extension. Serialisation of \texttt{.onnx} files is performed using Protobuf (Protocol Buffers), a language and platform-neutral mechanism for serializing structured data. 

Key data stored includes:
\begin{itemize}
	\item \textbf{Metadata}: This describes attributes of the model such as model version, author, and description.
	\item \textbf{Initializers}: A list of constant tensors that are used by the model, such as weights and biases.
	\item \textbf{Inputs}: A list of descriptions of the one or more tensors that the model expects as input. This includes a name, the type of data stored in the tensor, and the shape of the tensor.
	\item \textbf{Nodes}: A list of nodes in the graph each representing a single operation on  tensors (e.g., convolution, activation functions). Each node has a list of input tensors and a list of output tensors representing its connections in the graph.
	\item \textbf{Outputs}: A list of descriptions of the one or more tensors that the model produces as output. This includes a name, the type of data stored in the tensor, and the shape of the tensor.
\end{itemize}


\paragraph{Tensors}
All tensors processed by ONNX models are strongly typed. The basic properties of an ONNX tensor include:
\begin{itemize}
	\item \textbf{Element Type}: ONNX defines a \href{https://onnx.ai/onnx/repo-docs/IR.html#tensor-element-types}{standard set of types}. 
	The most commonly used types for neural network verification are: 32-bit floating point (float32), and 16-bit floating point (float16). Signed and unsigned integers (e.g., int32, uint8) are also supported, but are less commonly used.
	\item \textbf{Shape}: The shape of a tensor is defined as a list of integers, where each integer represents the size of the corresponding dimension. For example, a tensor with shape [3, 224, 224] 
	represents an image with 3 color channels (RGB) and dimensions 224\(\times\)224 pixels.
	\item \textbf{Values}: A contiguous block of memory containing the values for the elements of the tensor.
\end{itemize}

\paragraph{Operators and Opsets}

Each node in the ONNX graph is an \emph{operator} which takes some number of input tensors and creates some number of output tensors. Each operator has the following meta-data: a name, a list of expected input tensors, a list of expected output tensors, and a list of attributes for configuring the operation (e.g. kernel size and stride for a convolution-based operator). The full list of ONNX operators can be found in the specification, but commonly used ones include:
\begin{itemize}
\item \textit{Gemm} (General Matrix Multiplication)
\item \textit{Conv} (Convolutional)
\item \textit{MaxPool} (Maximum pooling)
\item \textit{ReLU} (Rectified Linear Unit)
\item  \textit{Sigmoid} (Logistic Unit)
\item \textit{Softmax} (Softmax Unit)
\end{itemize}
ONNX uses a versioning system called \emph{opsets} to manage the evolution of the list of supported operators. Each opset supports a given set of operators, and the semantics of existing operators may change between opset versions.
This ensures that the definition and behaviour of each operator can evolve over time without breaking existing models. The ONNX model file declares the opset version it uses in its metadata.

\subsection{Semantics}
\label{sec:onnx-semantics}

As described above, the ONNX standard provides a syntax for specifying a neural network mdoel. However, to formally define a satisfiability query over such a model, requires a precise, mathematical description of the semantics of the model, i.e. the computation that the model represents. 

Unfortunately, like most other mainstream neural network formats, ONNX has no formal mathematical semantics. 
Instead, the intended behavior of each operator is described informally in natural language by the ONNX documentation, in varying levels of detail and clarity. 
As a result, the exact computation may differ across implementations or hardware platforms.

Encouragingly, progress is being made: since 2024, the \emph{ONNX Safety-Related Profile} working group has been developing a restricted subset of ONNX with formal semantics. 
Work on this is currently ongoing.

\section{Network Theories}
\label{sec:network-theory}

In Chapter~\ref{sec:specification_language}, we will define the syntax and semantics of the \vnnlib{} query language for describing satisfiability problems over ONNX models of neural networks.
The goal of a satisfiability query is to reason about the behaviour of the function that the ONNX model represents (i.e. its semantics).
Therefore, clearly it is unavoidable that the syntax and semantics of a query will depend on the syntax and semantics of the ONNX models.

This dependence on the ONNX standard has two immediate consequences for \vnnlib{}:
\begin{enumerate}
\item The current absence of a formal semantics for ONNX described in Section~\ref{sec:onnx-semantics}, means that it is not currently possible to define the semantics of \vnnlib{} concretely.
\item Even if there was a formal semantics for ONNX available, both the syntax and semantics of the ONNX standard will continue to evolve. 
It would be undesirable to tie the \vnnlib{} standard to a specific version of ONNX, as that would require new versions of the \vnnlib{} standard to be released in lockstep with the ONNX standard.
\end{enumerate}
The solution to these problems is define the syntax and semantics of \vnnlib{} \emph{relative} to some abstract theory of neural networks.
In practice, this means the \vnnlib{} query language will be parameterised by an abstract signature $\networkTheoryVar$ containing the minimal set of syntax, typing judgments and semantics for neural networks necessary to define the syntax and semantics of the query language.
We will call the signature $\networkTheoryVar$ a \emph{network theory} . 

Therefore, although each new version of the ONNX standard will define a new network theory, complete with its own operators, element types and semantics, the \vnnlib{} standard will remain constant relative to an abstract theory $\networkTheoryVar$.
This allows the ONNX standard to evolve independently without requiring the explicit redefinition of the syntax and semantics of \vnnlib{}\footnote{
Note that nothing in the definition of a network theory is explicitly dependent on ONNX. 
The same mechanism could be used to define the syntax and semantics relative to other frameworks e.g. PyTorch. 
However, for the reasons outlined at the start of this chapter, in practice the \vnnlib{} standard assumes the use of the ONNX format.
}.

\begin{figure}
	\newcommand{\tensorOpOneSem}[2]{ 
		\semTensorType(#1) \rightarrow 
		\semTensorType(#2)
	}
	\newcommand{\tensorOpTwoSem}[3]{
		\semTensorType(#1) \rightarrow 
		\semTensorType(#2) \rightarrow 
		\semTensorType(#3) 
	}
	\newcommand{\compSem}[1]{
	\sem{#1} 
	&: \forall \tensorTypeVar. \; 
			\semTensorType(\tensorTypeVar) \rightarrow
			\semTensorType(\tensorTypeVar) \rightarrow
			\mathbb{B}
	&= \missing 
	}
	\newcommand{\opOneSem}[1]{
	\sem{#1}
	&: \forall \tensorTypeVar. \; 
			\tensorOpOneSem{\tensorTypeVar}{\tensorTypeVar}
	&= \missing 
	}
	\newcommand{\opTwoSem}[1]{
	\sem{#1} 
	&: \forall \tensorTypeVar. \; 
			\tensorOpTwoSem{\tensorTypeVar}{\tensorTypeVar}{\tensorTypeVar}
	&= \missing 
	}
	
	\begin{subfigure}{\textwidth}
	\setlength{\grammarindent}{9em}
	\begin{grammar}	
	<elementType> $\ni \elementTypeVar$ ::= \missing	
	
	<shape> $\ni \shapeVar$ ::= [$n_1$,...,$n_k$] \hspace{1em} for $n_i \in \mathbb{N}$ 
	
	<tensorType> $\ni \tensorTypeVar$ ::= <elementType>  $\times$ <shape>
	
	<modelType> $\ni \modelTypeVar$ ::= [<tensorType>]$^+$ $\times$ [<tensor-type>]$^+$
	\end{grammar}
	\caption{Abstract grammar for network types}
	\label{fig:onnx-type-syntax}
	\end{subfigure}
	\\
	\\
	
	\begin{subfigure}{\textwidth}
	\centering
	\begin{minipage}[t]{0.44\textwidth}
	\begin{grammar}	
	<element> $\ni \elementVar$ ::= (-)[0-9]$^+$(.[0-9]$^+$)
	\end{grammar}
	\end{minipage}
	\hfill
	\begin{minipage}[t]{0.23\textwidth}
	\begin{grammar}	
	<tensor> $\ni \tensorVar$ ::= \missing
	\end{grammar}
	\end{minipage}
	\hfill
	\begin{minipage}[t]{0.24\textwidth}
	\begin{grammar}
	<model> $\ni \modelVar$ ::= \missing
	\end{grammar}
	\end{minipage}
	\vspace{0.8em}
	\caption{Abstract grammar for network expressions}
	\label{fig:onnx-expr-syntax}
	\end{subfigure}
	\\
	
	\begin{subfigure}{\textwidth}
	% This minipage exists to force left-alignment
	\begin{minipage}[t]{0.5\textwidth}
	\begin{flalign*}
		&\inferrule*[Right=(Element)]{
            \missing
        }{
            \vdash \elementVar : \elementTypeVar
        }
        \hspace{8em}
        \inferrule*[Right=(Tensor)]{
            \missing 
        }{
            \vdash \tensorVar : \tensorTypeVar
        }
        \hspace{8em}
	    \inferrule*[Right=(Model)]{
            \missing
        }{
            \vdash \modelVar : \modelTypeVar 
        }
    \end{flalign*}	
	\end{minipage}
	\caption{Abstract type system}
	\label{fig:onnx-types}
	\end{subfigure}
	\\
	
	\begin{subfigure}{\textwidth}
	% This minipage exists to force left-alignment
	\begin{minipage}[t]{0.5\textwidth}
	\setlength{\arraycolsep}{2pt}
	\begin{equation*}
	\begin{array}{llll}
	\semElementType 
	&: \mgrammar{<elementType>}
	&\rightarrow \set
	&= \missing 
	\\
	\semTensor
	&: (\vdash \mgrammar{<tensor>} : \tensorTypeVar )
	& \rightarrow \semTensorType(\tensorTypeVar)
	&= \missing 
	\\
	\semModel
	&: (\vdash \mgrammar{<network>} : \modelTypeVar) 
	& \rightarrow \semModelType(\modelTypeVar) 
	&= \missing
	\end{array}
	\end{equation*}
	\begin{equation*}
	\begin{array}{llll}
	\compSem{\leq}
	\\
	\compSem{<}
	\\
	\compSem{\geq}
	\\
	\compSem{>}
	\\
	\compSem{=}
	\\
	\compSem{\neq}
	\\
	\opOneSem{-}
	\\
	\opTwoSem{+}
	\\
	\opTwoSem{\times}
	\end{array}
	\end{equation*}
	\begin{equation*}
	\begin{array}{llll}
	\semTensorType 
	&: ((\elementTypeVar, s))
	& \rightarrow \set
	&= (\semElementType(\elementTypeVar))^{\prod_i s_i}
	\\
	\semModelType 
	&: ((\tensorTypeVar^I, \tensorTypeVar^O))
	& \rightarrow \set 
	&= \prod_{i} \semTensorType(\tensorTypeVar^I_i)  \rightarrow
	\prod_{i} \semTensorType(\tensorTypeVar^O_i)
	\end{array}
	\end{equation*}
	\end{minipage}
	\caption{Abstract semantics}
	\label{fig:onnx-semantics}
	\end{subfigure}
	\caption{The definition of a network theory $\networkTheoryVar$, i.e. the minimal signature for an abstract implementation of ONNX that allows the syntax and semantics of VNN-LIB to be defined. The \missing{} symbol indicates what needs to be defined by the ONNX standard to instantiate the theory.}
	\label{fig:onnx-signature}
\end{figure}


The definition of a network theory is shown in Figure~\ref{fig:onnx-signature}, with the missing components to be instantiated represented using the \missing{} symbol.
Figures~\ref{fig:onnx-type-syntax}~\&~\ref{fig:onnx-expr-syntax} define the syntax of the types and expressions:
\begin{enumerate}
\item $\mgrammar{<elementType>}$ - a set of numeric element types, e.g. the element types \texttt{float64}, \texttt{int32} in ONNX.
\item $\mgrammar{<tensor>}$ - the format used to represent tensors, e.g. the \texttt{TensorProto} object in ONNX.
\item $\mgrammar{<model>}$ - the format used to represent neural network models, e.g. the \texttt{ModelProto} object in ONNX.
\end{enumerate}
Figure~\ref{fig:onnx-types} defines the required typing judgments over the syntax:
\begin{enumerate}
\item \textsc{(Element)} - a judgement that a given numeric string is a valid element of a provided element type. For instance, it would be expected that the number `1' would be judged as of type \texttt{float64} and \texttt{int32}, and `1.0' would be judged as of type \texttt{float64} but not \texttt{int32}.
\item \textsc{(Tensor)} - a judgement that a given tensor is of a given element type and shape.
\item \textsc{(Model)} - a judgment that a given model takes in a list of tensors of the provided shape and produces a list of output tensors of the required shapes.
\end{enumerate}
Finally~\ref{fig:onnx-semantics} defines the semantics over the well-typed syntactic constructs:
\begin{enumerate}
\item $\semElementType$ - a function mapping each syntactic element type to a mathematical set of values.
\item $\semTensor$ - a function mapping each syntactic tensor to a mathematical tensor of the required type and shape.
\item $\semModel$ - a function mapping each syntactic model to an executable function from a list of input mathematical tensors of the required shape to a list of output mathematical tensors of the required shape.
\item $\sem{\leq}$, ..., $\sem{\times}$ - a list of functions that perform basic pointwise comparison and arithemetic operations over tensors.
\end{enumerate}
Note that one of the consequences of this parameterisation is that when discussing the semantics of a particular \vnnlib{} query, it is necessary to state the versions of both the ONNX standard and the \vnnlib{} standard being used.