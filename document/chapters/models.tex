\chapter{Network Models}
\label{sec:models}
%
A core component of the \vnnlib{} specification is the choice of the ONNX format as the standard format for representing neural network models. Fundamentally, a neural network can be viewed as a directed computation graph, where nodes correspond to operations (e.g., addition, multiplication, activation functions) over tensors, and edges represent the flow of data between these operations. Inference consists of applying these operations sequentially to one or more input tensors, propagating values through the graph, and producing one or more output tensors.

To formally reason about whether a query about a neural network is satisfiable, a standardised representation of such computation graphs is essential. Many formats exist for serialising neural networks, each serving different use cases—ranging from deployment and interoperability to training and debugging. Training-framework-native formats, such as TensorFlow’s .ckpt or PyTorch’s .pth files, are designed for use within specific ecosystems. In contrast, interoperable formats aim to decouple model representation from a particular framework or hardware backend, enabling broader reuse and tool support. Notable examples include ONNX, NNEF, GGUF, and Safetensors.

Among these, ONNX (Open Neural Network Exchange) has been chosen as the standard format for neural networks in \vnnlib{} for the following reasons:
\begin{itemize}
\item \textbf{Community owned}: ONNX is a community-driven rather than a proprietary project. This reduces the risk of the discontinuation of the standard and ensures greater neutrality and transparency in its governance.
\item \textbf{Framework and Hardware Agnostic}: ONNX is designed to be independent of both training frameworks and deployment environments. This makes it well-suited to serve as a common exchange format across diverse toolchains and platforms.
\item \textbf{Widespread Adoption}: ONNX is widely supported by major frameworks, including PyTorch and TensorFlow, which provide tooling for exporting to and importing from the ONNX format. This facilitates model sharing and conversion.
\item \textbf{Rich and Extensible Operator Set}: ONNX includes a comprehensive set of standardised operators, enabling it to express a wide variety of neural network architectures. Its extensibility also allows new operators to be proposed and adopted by the community as needed.

\item \textbf{Versioning and Documentation}: The ONNX specification includes detailed documentation and operator definitions, and a robust versioning system. This makes it easier for tools to implement and interact with the format in a consistent manner.

\item \textbf{Existing Verifier Ecosystem}: Finally, ONNX is already well-integrated into the neural network verification research community. It has served as the standard format in the VNN-COMP competition, making it a natural fit for \vnnlib{} and aligning with existing verifier tooling.
\end{itemize}

\section{Overview of the ONNX Format}
\label{sec:onnx_overview}

We will now give a brief overview of the ONNX format. For a more detailed description suitable for implementators of neural network verifiers, the full ONNX specification should be consulted. 

\paragraph{Model structure} 

An ONNX model is serialised as a single binary file with the `.onnx' file extension. The serialisation is performed using Protocol Buffers (Protobuf), a language and platform-neutral mechanism for serializing structured data. Key data stored within the model includes:
\begin{itemize}
	\item \textbf{Metadata}: This describes attributes of the model such as model version, author, and description.
	\item \textbf{Initializers}: A list of constant tensors that are used by the model, such as weights and biases.
	\item \textbf{Inputs}: A list of descriptions of the one or more tensors that the model expects as input. This includes a name, the type of data stored in the tensor, and the shape of the tensor.
	\item \textbf{Nodes}: A list of nodes in the graph each representing a single operation on  tensors (e.g., convolution, activation functions). Each node has a list of input tensors and a list of output tensors representing its connections in the graph.
	\item \textbf{Outputs}: A list of descriptions of the one or more tensors that the model produces as output. This includes a name, the type of data stored in the tensor, and the shape of the tensor.
\end{itemize}


\paragraph{Tensors}
All tensors processed by ONNX models are strongly typed. The basic properties of an ONNX tensor include:
\begin{itemize}
	\item \textbf{Element Type}: ONNX defines a \href{https://onnx.ai/onnx/repo-docs/IR.html#tensor-element-types}{standard set of types}. 
	The most commonly used types for neural network verification are: 32-bit floating point (float32), and 16-bit floating point (float16). Signed and unsigned integers (e.g., int32, uint8) are also supported, but are less commonly used.
	\item \textbf{Shape}: The shape of a tensor is defined as a list of integers, where each integer represents the size of the corresponding dimension. For example, a tensor with shape [3, 224, 224] 
	represents an image with 3 color channels (RGB) and dimensions 224\(\times\)224 pixels.
	\item \textbf{Values}: A contiguous block of memory containing the actual data.
\end{itemize}

\paragraph{Operators and Opsets}

Each node in the ONNX graph is an \emph{operator} which takes some number of input tensors and creates some number of output tensors. Each operator has the following meta-data: a name, a list of expected input tensors, a list of expected output tensors, and a list of attributes for configuring the operation (e.g. kernel size and stride for a convolution-based operator). The full list of ONNX operators can be found in the specification, but commonly used ones include: 
\textit{Gemm} (General Matrix Multiplication), \textit{Conv} (Convolutional), \textit{MaxPool} (Maximum pooling), 	\textit{ReLU} (Rectified Linear Unit), \textit{Sigmoid} (Logistic Unit), \textit{Softmax} (Softmax Unit).

ONNX uses a versioning system called \emph{opsets} to manage the evolution of the list of supported operators. Each opset version supports a given set of operators, and the semantics of existing operators may change between opset versions.
This ensures that the definition and behaviour of each operator can evolve over time without breaking existing models. The ONNX model file declares the opset version it uses in its metadata.

\section{Semantics of ONNX networks}

The ONNX format provides a syntax for specifying the computations performed by a neural network. However, formal verification requires more than syntax—it requires a precise, mathematical understanding of the network’s semantics. Unfortunately, no mainstream neural network format, including ONNX, currently offers a formal semantics. The intended behavior of each operator is described informally in natural language by the ONNX documentation, in varying levels of detail and clarity. As a result, the exact computation may differ across implementations or hardware platforms.

This lack of formal semantics presents challenges for verification, but the community has so far accepted this limitation as a practical compromise. Encouragingly, progress is being made: since 2024, the \emph{ONNX Safety-Related Profile} working group has been developing a restricted subset of ONNX with formally specified semantics. Work is currently ongoing.

\section{Verifier support for ONNX in practice}
\label{sec:onnx_support}

Currently, we are not aware of any neural network solver that supports the full range of ONNX models. In practice, a particular solver will handle only a subset of ONNX operators, data types, and model structures.
To promote clarity and interoperability, the \vnnlib{} command-line interface includes a mechanism for querying the extent of a particular verifier's support for the ONNX standard (see Section~\ref{sec:global_capabilities}). This mechanism allows verifiers to explicitly report the ONNX features they support, enabling toolchains and users to reason more effectively about compatibility.