\chapter{Query Language}\label{sec:specification_language}

The SMT-LIB language is a well-known language used to formalize 
Satisfiability Modulo Theories problems, and is expressive enough to
represent the verification properties of interest. In this language, 
it is possible to define both the \textit{pre-} and 
\textit{post-}conditions at once, by defining the variables for the
input and the output of the neural network. In the following we
show some examples of networks and corresponding properties in the
SMT-LIB language.

\section{Syntax}
\label{sec:syntax}


The syntax of VNN-LIB is formally defined using Labelled Backus-Naur Form (LBNF)\cite{8}. LBNF is a variant of BNF that allows for 
annotations (labels) on productions, facilitating the automatic generation of abstract syntax trees, parsers, and other language processing tools. 
This formal grammar provides a rigorous foundation for the language, eliminating ambiguities present in previous versions and ensuring consistent 
parsing across different tools.

The full LBNF grammar for VNN-LIB is provided in the Appendix~\ref{}. The following subsections highlight key syntactic constructs of the language,
with examples illustrating their usage.

\subsection{Network declarations}

A network declaration is introduced by the keyword \texttt{declare-network}, followed by a user-defined variable name for the network, 
and then its associated input, hidden (hidden), and output variable declarations. All variables are declared inside of network declarations and variable 
names must be unique within the scope of the entire VNN-LIB specification.

\begin{lstlisting}[
    caption=Network Definition Example, 
    style=lbnf,
    label={lst:network-definition}
]
(declare-network acc
    (declare-input X Real 3)
    (declare-hidden N Real 1 2 onnx-node:"node_name_in_onnx")
    (declare-output Y Real 3)
) 
\end{lstlisting}
\mnote{What does `acc' mean? Is there a better example name?}
\mnote{I would remove the hidden node in this initial example. Let's start off simple!}
\mnote{Also for all of these it would be great to have a picture of a tiny ONNX network that satisfies them on the RHS. You can build the models in Python and visualise them with https://netron.app/ }
\mnote{We should also say that the variable names are used in the query, but the network name is not used in the query but to link the networks up to the verifier. Again with suitable forward references.}

For instance, Listing~\ref{lst:network-definition} declares a network named \texttt{acc}, with an input tensor \texttt{X}, hidden tensor \texttt{N}, 
and output tensor \texttt{Y}. 
\mnote{Can call these "Example" rather than "Listing"? And this text should probably go into the caption rather than below.}



\paragraph{Input and Output Variable Declarations}
An input variable is declared using the \texttt{declare-input} keyword, followed by a variable name, its element type (e.g., \texttt{Real}, \texttt{int8}), 
and a space-seperated list of integers representing the shape of the tensor. Similarly, an output variable uses the \texttt{declare-output} keyword. Multiple 
input and output variables can be declared within a single network declaration, and the order of declaration determines the mapping to the nodes in the ONNX model.
The variables may be explicitly associated with ONNX node names using the \texttt{onnx-node} keyword and a string identifer, only under the condition that all input 
and output variables are associated with an ONNX node name.

\begin{lstlisting}[
  caption=Input and Output Definition Example,
  style=lbnf,
  label={lst:input-output}
]
(declare-input X Real 1 28 28) 
\end{lstlisting}
\mnote{Can this example be the full network declaration with multiple inputs and outputs?}
\mnote{Can you then give a second equivalent definition for the same network using the ONNX names instead?}
\mnote{This section also needs to list the rules for what are valid variable names?}

For example, Listing~\ref{lst:input-output} declares an input tensor named \texttt{X} of real numbers with shape $1 \times 28 \times 28$. 

\paragraph{Hidden Node Declarations}
A hidden node is declared using the \texttt{declare-hidden} keyword. This declaration includes a variable name for use within the VNN-LIB specification, 
its element type, its tensor shape, and crucially, a string identifier that specifies the corresponding node name in the ONNX graph. The ability to declare hidden nodes
allows for properties to reference key intermediate computations within the network, such as encoding features, attention mechanisms, or other internal states. Multiple
hidden nodes can be trivially declared within a single network declaration.

\begin{lstlisting}[
    caption=hidden Node Declaration Example, 
    style=lbnf,
    label={lst:hidden-node}
]
(declare-hidden H1 Real 100 onnx-node:"layer3/relu_out") 
\end{lstlisting}
\mnote{Again, the full network declaration with an example network on the RHS would be great?}

Listing~\ref{lst:hidden-node} declares an hidden variable \texttt{H1} of type \texttt{Real} with a shape of `(100)`. The \texttt{onnx-node} attribute 
specifies that this variable corresponds to the tensor named \texttt{"layer3/relu\_out"} in the associated ONNX graph. 

\paragraph{Multiple networks}

VNN-LIB supports the definition of more than one neural networks within a single specification file. This is crucial for properties that need to refer to
multiple networks (e.g. ...). 

\mnote{TODO, would be good to have an example with multiple networks}

\subsection{Assertion declarations}
VNN-LIB supports quantifier-free logic formulas. Assertions are defined using parenthesised \texttt{(assert\ldots)} expressions, and follows an SMT-LIB-like syntax with the 
operand preceding its arguments. The operands can be logical operators (e.g., \texttt{and}, \texttt{or}) or arithmetic operators (e.g., \texttt{+}, \texttt{-}, \texttt{*}).
An assertion is a logical formula that consists of logical and arithmetic operations over one or more elements of the declared tensors.

\paragraph{Variables}
Let $X \in I$ be an $n$-dimensional tensor in some generic input domain $I = I^{d_1 \times \cdots \times d_n}$. The ``matrix notation'' represents a specific 
element $x_{i_1, i_2, \dots, i_n}$ of the tensor $X$ as \texttt{X\_$i_1$-$i_2$-\dots-$i_n$}, where $i_1, \dots, i_n$ are the indices of the element of interest in the 
dimensions $d_1, \dots, d_n$. To better clarify, if we consider the 1-D tensor $X \in I^n$, the 2-D tensor $Y \in I^{n \times m}$, and the 3-D tensor 
$Z \in I^{n \times m \times p}$, we will have the following representations:
\begin{itemize}
    \item \texttt{X\_0}, \texttt{X\_1}, \dots, \texttt{X\_$i$}, \dots, \texttt{X\_$n$};
    \item \texttt{Y\_0-0}, \texttt{Y\_0-1}, \dots, \texttt{Y\_$i$-$j$}, \dots, \texttt{Y\_$n$-$m$};
    \item \texttt{Z\_0-0-0}, \texttt{Z\_0-0-1}, \dots, \texttt{Z\_$i$-$j$-$k$}, \dots, \texttt{Z\_$n$-$m$-$p$};
\end{itemize}
In such a representation, \texttt{Z\_$i$-$j$-$k$} corresponds to the element $z_{i,j,k}$ of the tensor $Z$. 
\mnote{Did we really go for dashes in betweens the indices? Why could we not use underscores everywhere?}

\paragraph{Arithmetic expressions}
\mnote{Discuss the supported arithmetic combinators}

\paragraph{Boolean assertions}
\mnote{Discuss the supported boolean combinators}

For example, Listing~\ref{lst:assertion-example} asserts that for a given range of the input neuron $A_1$, the output neuron $B_0$ 
is greater than another output neuron $B_1$. More complex properties, including those referencing multiple networks or hidden nodes, 
can be constructed using these foundational elements.

\begin{lstlisting} [
	caption=Assertion Example, 
	style=lbnf,
    label={lst:assertion-example}
]
(assert 
    (and 
        (and (>= A_0 0.0) (<= B_0 1.0)) 
        (> B_0 B_1)
    )
)
\end{lstlisting}
\mnote{Talk about whether whitespace is important or not?}

\section{Scoping}
\label{sec:scoping}

TODO Ann

\section{Typing}
\label{sec:typing}

TODO Ann

\section{Semantics}
\label{sec:semantics}

TODO Ann

