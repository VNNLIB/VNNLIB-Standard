\chapter{Query Language}
\label{sec:specification_language}

The centre-piece of the \vnnlib{} standard is the \vnnlib{} query language which is heavily inspired by the SMT-LIB query language and is designed as a standardised computer-readable format for expressing satisfiability problems over neural networks.
Although the syntax is somewhat human-readable, the query language is designed to primarily optimise machine-readability. 
It is expected that \vnnlib{} queries will primarily be generated automatically by higher level tools that provide more human-friendly interfaces.
This chapter describes the syntax, scoping, typing and semantics of the query language. Unless explicitly stated otherwise, everything in this chapter is implicitly parameterised by some underlying network theory $\networkTheoryVar$, the definition of which was provided in Section~\ref{sec:network-theory}.

\section{Syntax}
\label{sec:syntax}

\begin{figure}
	\setlength{\grammarindent}{6.5em}
	
	\begin{grammar}
	<query> ::= <version> [<network>]$^+$ [<assert>]$^*$
	
	<version> ::= (\texttt{vnnlib-version} [0-9]$^+$.[0-9]$^+$)
	
	<name> ::= [A-Za-z][\_A-Za-z0-9]*
		
	<network> ::= (\texttt{declare-network} <name> <equiv>$^?$ <input>$^+$ <hidden>* <output>$^+$)
	
	<equiv> ::= (\texttt{equal-to} <name>) | (\texttt{isomorphic-to} <name>)
	
	<input> ::= (\texttt{declare-input} <name> <elementType>$\networkTheoryVarParam$ <shape>)
	
	<hidden> ::= (\texttt{declare-hidden} <name> <elementType>$\networkTheoryVarParam$ <shape> <name>)
	
	<output> ::= (\texttt{declare-output} <name> <elementType>$\networkTheoryVarParam$ <shape>)
	
	<assert> ::= (\texttt{assert} <bool>)
	
	<bool> ::= (\texttt{and} <bool> <bool>$^+$)
	\alt (\texttt{or} <bool> <bool>$^+$)
	\alt (\texttt{+} <arith> <arith>)
	\alt (\texttt{\textless} <arith> <arith>)
	\alt (\texttt{\textless=} <arith> <arith>)
	\alt (\texttt{\textgreater} <arith> <arith>)
	\alt (\texttt{\textgreater=} <arith> <arith>)
	\alt (\texttt{==} <arith> <arith>)
	\alt (\texttt{!=} <arith> <arith>)
	
	<arith> ::= <element>$\networkTheoryVarParam$
	\alt <name> <indices>$^?$
	\alt (\texttt{-} <arith>)
	\alt (\texttt{+} <arith> <arith>$^+$)
	\alt (\texttt{*} <arith> <arith>$^+$)
	\alt (\texttt{-} <arith> <arith>$^+$)
	
	<indices> ::= [i, ..., i] for $i \in \mathbb{N}$
	\end{grammar}
	\vspace{-1em}
    \caption{The syntax of \vnnlib{} queries, parameterised by the syntax of the underlying network theory $\networkTheoryVar$ described in Figures~\ref{fig:onnx-type-syntax}~\&~\ref{fig:onnx-expr-syntax}. As is convention, $^*$ means zero or more, $^+$ means one or more, and $^?$ means optional.}
    \label{fig:vnnlib-syntax}
\end{figure}

The syntax of the \vnnlib{} query language is shown in Figure~\ref{fig:vnnlib-syntax}.  This section will now describe the key syntactic constructs of the language via illustrative examples. Firstly, queries are split into three parts: 
\begin{enumerate}
\item \textbf{Version number} - the version of the VNN-LIB the query is using. This allows tools to provide better diagnostic error messages.
\item \textbf{Network declarations} - a non-empty list of network declarations. These describe the networks that involved in the satisfiability problem and allow the introduction of new abstract variables that represent the values for the inputs and outputs of those networks.
\item \textbf{Assertions} - a list of assertions that reference the variables introduced by the network declarations, thereby defining the constraints that should be satisfied.
\end{enumerate}
Currently, the standard does not permit the interleaving of network declarations and assertions. Figure~\ref{fig:simple-query} shows an example of a simple query.


\subsection{Network Declarations}
\label{sec:network-declarations}

A network is introduced by the keyword \texttt{declare-network}, followed by a user-defined name for the network, and then declarations for its associated input and output. 
An input is declared using the \texttt{declare-input} keyword, followed by a variable name, an element type from the associated \networkTheory{} (e.g., \texttt{float64}, \texttt{int32}), 
and the shape of the tensor. Similarly, an output variable uses the \texttt{declare-output} keyword. In the case of Figure~\ref{fig:simple-query}, the network is named \texttt{myNetwork}, and it has one input called \texttt{X} consisting of a $1 \times 10$ tensor of ONNX \texttt{float32} values and one output called \texttt{Y} consisting of a $1 \times 2$ tensor of ONNX \texttt{float32} values. 
\begin{figure}[t]
    \begin{minipage}[c]{0.62\textwidth}
        \begin{lstlisting}[style=lbnf]
(declare-network myNetwork
    (declare-input  X float32 [1,10])
    (declare-output Y float32 [1,2])
)

(assert (>= X[0,2] 0.0))
(assert (<= X[0,2] 1.0))
(assert (<= Y[0,1] 0.5))\end{lstlisting}
    \end{minipage}%
    \begin{minipage}[c]{0.35\textwidth}
        \centering
        \includegraphics[height=5cm]{imgs/simple_net.onnx.pdf}
    \end{minipage}
    \caption{A simple \vnnlib{} specification which declares a network with a single input and output tensor. An example of one of the many possible ONNX models compatible with this declaration is shown on the right. Note that the variable names \texttt{X} and \texttt{Y} for the declared inputs and outputs do not have to match the node names in the ONNX model.}
    \label{fig:simple-query}
\end{figure}


All variable names follow the same syntax conventions: they are case-sensitive, must start with a letter, and may only contain letters, digits and underscores. All variable names must be unique within the query (see Section~\ref{sec:scoping_and_typing} for more detailed scoping rules). 

It is important to clarify that a network declaration only declares an \emph{interface} and does not reference any particular ONNX network model. 
When passing a query and a network model to the verifier, the name of the network does not need to match the name of the ONNX model file. 
Instead, as described in Section~\ref{sec:verify_command}, the user will explicitly associate a network declaration to the relevant model file via the command line.
Neither do the declared names of the inputs and outputs need to match the names of the input and output nodes within the ONNX model file. This flexibility allows many different alternative ONNX models to be compatible with the same query.

Equally, the network declarations do not constrain the internal architecture of the model, including which operators (e.g. Gemm, ReLU, Softmax) are used. In practice, different verifiers will support different operators and Section~\ref{sec:onnx-capabilities} provides a mechanism for a verifier to report its own capabilities in this regard.

% The \texttt{@} character is a reserved character which is used to denote multiple applications of the same network, for the purpose of defining  hyperproperties such as monotonicity. For example \texttt{(declare-network acasXu@1 ...)} and \texttt{(declare-network acasXu@2 ...)} define two networks that are both instances of the same ONNX model,  denoted as \texttt{acasXu} in the command line interface of the solver (See Chapter~\ref{sec:solver_interface} for more details).

\subsection{Assertions}

The list of assertions constrain the variables introduced by the network declarations. \vnnlib{} assertions are quantifier-free logical formulas and are defined using parenthesized \texttt{(assert\ldots)} expressions. 
They use an SMT-LIB-like reverse Polish notation syntax with the operator preceding its operands.
An assertion is a logical formula that may include logical connectives, relational comparisons, and arithmetic expressions over declared tensors and constants.
The final satisfiability problem is then the conjunction of all the assertions.
 
\paragraph{Constants}

Numeric constant values may be referenced using basic standard integer or floating point syntax (e.g. \inlinevnn{0}, \inlinevnn{0.0}, \inlinevnn{-0.5}). 

\paragraph{Variables} 

Standard indexing notation may be used to refer to a specific element within the tensor variable. For example, Line 6 in Figure~\ref{fig:simple-query} uses \inlinevnn{X[0,2]}, to refer to the value of the element of the input tensor at row~0, column~2. All indices are zero based and currently the number of indices provided must be equal to the number of dimensions of the variable, i.e. partial indexing is not allowed.

\paragraph{Arithmetic expressions}

One forms arithmetic expressions by recursively combining constant and variable values via prefix notation. Currently the following operators are supported:
\begin{itemize}
	\item \inlinevnn{(- a)}: Negation of a term.
    \item \inlinevnn{(+ a b ...)}: Addition of two or more terms. 
    \item \inlinevnn{(* a b ...)}: Multiplication of two or more terms. 
    \item \inlinevnn{(- a b ...)}: Subtraction of two or more terms. Note that subtraction associates to the left, e.g. \inlinevnn{(- 1 2 3 4)} is the same as \inlinevnn{(- (- (- 1 2) 3) 4)} .
\end{itemize}
Arithmetic operations currently only operate over individual tensor elements and cannot be used to operate over multi-dimensional tensors (see Section~\ref{sec:scoping_and_typing} for detailed typing rules). 

Although some ONNX types can be safely coerced between in certain cases (e.g. a \inlinevnn{float16} can be added safely to a \inlinevnn{float32} if the expected type of evaluated expression is a \inlinevnn{float32}), the \vnnlib{} standard does not permit such coercions.
All variables within an arithmetic expression must have the same type (see Section~\ref{sec:scoping_and_typing} for formal typing rules). 

\paragraph{Comparisons}

The comparison operators \inlinevnn{<=}, \inlinevnn{>=}, \inlinevnn{<}, \inlinevnn{>}, \inlinevnn{=}, \inlinevnn{!=} can be used to compare the values of two arithmetic operations.
For example, \inlinevnn{(<= a b)} returns true if $a$ is less than or equal to $b$.
    
\paragraph{Boolean expressions} 

One forms boolean expressions by recursively combining comparisons via the following supported logical connectives:
\begin{itemize}
    \item \inlinevnn{(and a b ...)}: Conjunction of two or more terms.
    \item \inlinevnn{(or a b ...)}: Disjunction of two or more terms.
\end{itemize}

\subsection{Complex Network Declarations}
\label{sec:complex-networks-decls}

Although queries often relate the input of a single network to its output in some way, \vnnlib{} supports more complex queries. The neural network may have multiple input or output nodes, or the query may refer to intermediate hidden layers of the network or it may relate the behaviour of multiple networks. The query language supports all of these use cases.

\subsubsection{Multiple input and output declarations}

\begin{figure}[t]
    \centering
    \begin{lstlisting}[style=lbnf]
(declare-network multi_io_net
    (declare-input  image    float32 [1,3,224,224])
    (declare-input  metadata float32 [1,10])
    (declare-output bbox     int16   [1 4])
    (declare-output logits   float32 [1,1000])
)\end{lstlisting}

    \vspace{0.5cm}
    \includegraphics[width=\textwidth]{imgs/multi_io_net.onnx.pdf}
    \caption{A \vnnlib{} network declaration with multiple inputs/outputs.}
    \label{fig:multi-inputs-outputs}
\end{figure}

Neural network that accept multi-modal input or produce multi-modal output are becoming increasingly common~\cite{wang2021survey}. 
For example, the network in Figure~\ref{fig:multi-inputs-outputs} shows a model that takes both an image and meta-data about that image as inputs, and outputs both a bounding box around an identified object of interest and a probability distribution over the possible class that the object belongs to. 

The query language supports such networks by allowing a network declaration to declare an arbitrary number of input and output declarations. 
The declared inputs and outputs are mapped to the ONNX graph's inputs and outputs by matching the order of declaration in the query to the order of nodes in the ONNX model file.


\subsubsection{Hidden node declarations}
\label{sec:hidden-output-declarations}

In some use cases it is desirable to constrain the result of intermediate computation at the output of hidden nodes within the network. For example, when reasoning about the encodings in an encoder-decoder architecture or when reasoning about attention mechanisms. This can be achieved by declaring hidden nodes using the \texttt{declare-hidden} keyword. This declaration includes a variable name for use within the \vnnlib{} specification,  its element type, its tensor shape, and crucially, a string identifier that specifies the corresponding node name in the ONNX graph.  Multiple hidden nodes can be trivially declared within a single network declaration. Figure~\ref{fig:hidden-node} shows a \vnnlib{} network declaration with a hidden node.

\begin{figure}[h!]
    \begin{minipage}[c]{0.76\textwidth}
        \begin{lstlisting}[style=lbnf]   
(declare-network encoder
    (declare-input  X float64 [1,28,28])
    (declare-hidden Z float64 [1,128] "hidden")
    (declare-output Y float64 [1,10])
)\end{lstlisting}
    \end{minipage}%
    \begin{minipage}[c]{0.21\textwidth}
        \centering
        \includegraphics[height=6cm]{imgs/encoder_net.onnx.pdf}
    \end{minipage}
    \caption{A \vnnlib{} network declaration that declares a hidden node}
    \label{fig:hidden-node}
\end{figure}

The hidden node declaration crucially refers to the uniquely identified outputs of a node, rather than the node (or operator) itself. It declares that an output of the node is to be used as a variable in the \vnnlib{} query.

\subsubsection{Multiple networks}
\label{sec:multiple-network}

Often you may want to relate the behaviour of one neural network to that of another. Classic examples include: teacher-student networks where you try to train a smaller, more efficient network to mimic the output of the larger network, or observer-controller architectures.

\vnnlib{} supports defining multiple networks in by including multiple network declarations in the same query. Figure~\ref{fig:multiple-networks} 
shows an example which declares two networks representing a teacher and a student network.

\begin{figure}[ht]
    \begin{minipage}[c]{0.64\textwidth}
        \begin{lstlisting}[style=lbnf]
(declare-network teacher
    (declare-input  TX float32 [1,32])
    (declare-output TY float32 [1,2])
)

(declare-network student
    (declare-input  SX float16 [1,32])
    (declare-output SY float16 [1,2])
)\end{lstlisting}
    \end{minipage}
    \begin{minipage}[c]{0.35\textwidth}
        \centering
        \includegraphics[height=7cm]{imgs/student_net.onnx.pdf}
        \vspace{0.5cm} 
        \includegraphics[height=7cm]{imgs/teacher_net.onnx.pdf}
    \end{minipage}
    \caption{A pair of \vnnlib{} network declarations that allows the referencing of multiple networks within a single query.}
    \label{fig:multiple-networks}
\end{figure}

For some use cases involving multiple networks (e.g. proving monotonicity), each \texttt{declare-network} declaration is intended to map to the same network implementation. In this case, the \texttt{equal-to} declaration may be used as shown in Figure~\ref{fig:multiple-equal-networks} to inform the solver of this fact. An \texttt{equal-to} declaration must reference a network declaration which has identical \texttt{declare-input} and \texttt{declare-output} declarations (modulo the variable names). It \emph{is} possible to have a different list of \texttt{declare-hidden} declarations in the two networks. However if a \texttt{declare-hidden} declaration that references the same ONNX node is present in both networks then those \texttt{declare-hidden} declarations must be identical modulo the variable names. 
 Finally, marking a \texttt{declare-network} declaration as equal to one another means that it is not necessary to provide an implementation for that declaration when calling the solver (see Section~\ref{sec:verify_command} for details).
 
\begin{figure}[h!]
    \begin{minipage}[c]{0.64\textwidth}
        \begin{lstlisting}[style=lbnf]
(declare-network f
    (declare-input  A float32 [1,10])
    (declare-output B float32 [1,2])
)

(declare-network f-copy
    (equal-to f)
    (declare-input  C float32 [1,10])
    (declare-output D float32 [1,2])
)\end{lstlisting}
    \end{minipage}
    \begin{minipage}[c]{0.35\textwidth}
        \centering
        \includegraphics[height=5cm]{imgs/simple_net.onnx.pdf}
    \end{minipage}
    \caption{A pair of \vnnlib{} network declarations that reference the same ONNX model.}
    \label{fig:multiple-equal-networks}
\end{figure}

In other use cases involving multiple networks (e.g. proving local equivalence after retraining or after quantisation), each network declaration will be mapped to a network with the same graph structure but with different weights. In this case, the \texttt{isomorphic-to} declaration may be used as shown in Figure~\ref{fig:multiple-isomorphic-networks}. An \texttt{isomorphic-to} declaration must reference a network declaration which has identical list of \texttt{declare-input} and \texttt{declare-output} declarations, modulo the variable names and the element type. As with the \texttt{equal-to} declaration, it \emph{is} possible to have a different list of \texttt{declare-hidden} declarations in the two \texttt{declare-network} declarations. However if a \texttt{declare-hidden} declaration that references the same ONNX node is present in both then those \texttt{declare-hidden} declarations must be identical modulo the variable names and element types.

\begin{figure}[h!]
    \begin{minipage}[c]{0.64\textwidth}
        \begin{lstlisting}[style=lbnf]
(declare-network f
    (declare-input  A float32 [1,10])
    (declare-output B float32 [1,2])
)

(declare-network g
    (isomorphic-to f)
    (declare-input  C float32 [1,10])
    (declare-output D float32 [1,2])
)\end{lstlisting}
    \end{minipage}
    \begin{minipage}[c]{0.35\textwidth}
        \centering
        \includegraphics[height=5cm]{imgs/simple_net.onnx.pdf}
        \vspace{0.5cm} 
        \includegraphics[height=5cm]{imgs/simple_net.onnx.pdf}
    \end{minipage}
    \caption{A pair of \vnnlib{} network declarations that reference isomorphic ONNX models.}
    \label{fig:multiple-isomorphic-networks}
\end{figure}

There are two additional restrictions to their use:
\begin{enumerate}
\item A \texttt{declare-network} declaration must contain at most one \texttt{equal-to} or \texttt{isomorphic-to} declaration.
\item An \texttt{equal-to} or \texttt{isomorphic-to} declaration cannot reference another network declaration that also contains an \texttt{equal-to} or \texttt{isomorphic-to}. 
\end{enumerate}
Due to the transitivity of equality and isomorphism, these restrictions do not reduce the expressive power of this feature, and make the dependencies easier to track by the solvers.

While a solver may make use of the \texttt{equal-to} and \texttt{isomorphic-to} declarations internally to optimise its representation and reasoning about the provided networks, it is not required to.

\subsubsection{Networks that use zero-dimensional tensors}

It is permitted for network declarations to contain nodes that are declared to have zero dimensions.
In this case, the variables should be referenced without any indices, as shown in Line 8 below:
\begin{lstlisting}[style=lbnf]
(declare-network f
    (declare-input  X float32 [1,2])
    (declare-output Y float32 [])
)

(assert (>= X[0,0] 0.0))
(assert (<= X[0,1] 1.0))
(assert (<= Y 0.5))
\end{lstlisting}


\subsection{Comments and Whitespace}

Comments in \vnnlib{} are denoted by a semicolon (\texttt{;}) and extend to the end of the line. They are used for annotation, explaining logic, or providing additional context. Whitespace in \vnnlib{} is used to separate tokens and improve readability and can include spaces, tabs, and newlines. Additional whitespace beyond the minimal necessary to separate tokens is ignored.


\section{Typing}
\label{sec:scoping_and_typing}

This section describes the typing rules for \vnnlib{} queries. The rules provide a formal description of the constraints on the syntax described informally in Section~\ref{sec:syntax}, e.g. all variable names must be unique, variables of different types cannot appear in the same assertion. 

Crucial to the typing rules is the notion of the current \emph{context} $\assertCtx = (\networkCtx, \variableCtx)$ which consists of two parts. The first is the \emph{network context}:
\begin{equation*}
\networkCtx : \mgrammar{<name>} \rightarrow \mgrammar{<network>}
\end{equation*}
which is a mapping from network names to the corresponding network declarations. 
The second is the \emph{variable context}:
\begin{equation*}
\variableCtx : \mgrammar{<name>} \rightarrow \mgrammar{<tensorType>}\networkTheoryVarParam
\end{equation*}
which is a mapping from the declared input/hidden/output variables to their shape and type.

As one would expect, queries are typed  by first constructing the context $\assertCtx$ by iterating over the network declarations, and then using that context to type the globally-scoped assertions:
\input{diagrams/typing-rule-query.tex}
The following sections now describe the typing rules for network declarations and assertions.

\subsection{Networks}

Lists of network declarations $\networkDeclSet$ are typed as:
\input{diagrams/typing-rule-networks.tex}
where $\nilList$ and $::$ represent the standard nil and cons constructors for building up a list recursively.

The rule for typing a network declaration is as follows:
\input{diagrams/typing-rule-network.tex}
First it is checked that the network name is unique, next the variable context is updated by iterating sequentially through the input, hidden and output node declarations. 

The corresponding nil and cons rules for lists of node declarations are omitted for brevity, but are analagous to \textsc{Network-Nil} and \textsc{Network-Cons}. 
The rules for typing individual input, hidden and output declarations are as follows:
\input{diagrams/typing-rule-network-variable.tex}
i.e. as long as the name of the variable has not already been seen and is therefore unique, then each declaration  adds a corresponding entry to the $\variableCtx$.

\subsection{Assertions}

Assertions are type-checked within some variable context.
\input{diagrams/typing-rule-assertion.tex}
which simply defers to the typing of boolean expressions.

The boolean expressions consist of the n-ary operations \texttt{and} and \texttt{or}, and the six comparison operators which can be type-checked as follows:
\input{diagrams/typing-rule-boolExpr.tex}
Crucially, the comparison operator rule requires that the arithmetic expressions being compared must of the same type $\elementTypeVar$.

Arithmetic expressions consisting of the basic arithmetic operations can be type-checked as follows:
\input{diagrams/typing-rule-arith-ops.tex}
The typing rules for constants defers to the typing rule provided by the underlying network theory~$\networkTheoryVar$:
\input{diagrams/typing-rule-arith-constants.tex}
For example, one might expect that the constant \texttt{0.5} would be a member of the ONNX \texttt{float32} element type, but not of the ONNX \texttt{int32} element type.

Finally, the typing rules for variables ensure that the variable is of the correct type and the indices provided fit within the declared shape of the variable:
\input{diagrams/typing-rule-arith-variables.tex}
Crucially, therefore variables provide the source of typing for the whole arithmetic expression. 
One consequence of this is that comparisons that contain no variables (e.g. \texttt{(<= 0.0 1.0)}) have no canonical type and therefore are not considered well-formed.

\subsection{Models}
\label{sec:model-typing}

As input, verifiers will take a query $\queryVar$ and a list of ONNX models $\networkImplementationSet \in [\mgrammar{<model>}\networkTheoryVarParam]$, and it is important that the models passed conform to the network declarations. 
Using the typing judgement \textsc{Model} from the underlying network theory in Figure~\ref{fig:onnx-signature}, the following typing judgement $\queryVar \vdash \networkImplementationSet$ ensures that the network models $\networkImplementationSet$ are well typed with respect to the network declarations in the query $\queryVar$:
\input{diagrams/typing-rule-model.tex}
where the function $\networkDeclType : \mgrammar{<network>} \rightarrow \mgrammar{<modelType>}\networkTheoryVarParam$ that maps a network declaration to the overall type of the model that it expects is defined in the obvious way.

\mytodo{Need to make sure the models respect the equivalence constraints}
\subsection{Input Assignments}
\label{sec:input-assignment-typing}

As output, verifiers will produce either a judgement that the query $q$ is unsatisfiable or a concrete assignment of input tensors, $\networkInputSet \in [\mgrammar{<tensor>}\networkTheoryVarParam]$, that causes the query to be satisfied. 
Using the typing judgement \textsc{Tensor} from the underlying network theory in Figure~\ref{fig:onnx-signature}, the following typing judgment $\queryVar \vdash \networkInputSet$ ensures that the input assignment $\networkInputSet$ is well typed with respect to the network declarations in the query~$\queryVar$:
\input{diagrams/typing-rule-inputAssignment.tex}
where the function $\networkInputsType : \mgrammar{<network>} \rightarrow [\mgrammar{<tensorType>}\networkTheoryVarParam]$ that maps a network declaration to the types of the input tensors that it expects is defined in the obvious way.

\section{Semantics}
\label{sec:semantics}

This section will now define the semantics of the \vnnlib{} query language, i.e. a precise description of the mathematical satisfiability problem that a solver is attempting to answer when provided with a well-typed query $\queryVar$ and list of models~$\networkImplementationSet$.

\subsection{Environment}
\label{sec:semantic-context}

In order to determine whether or not a query $\queryVar$ over a list of network models $\networkImplementationSet$ is satisfied by a given input assignment $\networkInputSet$, it is necessary to determine the value of the declared network variables. 
This notion is captured formally by the \emph{environment} $\assertEnv$ which is a function:
\begin{equation*}
\assertEnv : (\nameVar : \tensorTypeVar) \rightarrow \semTensorTheory{\delta}
\end{equation*}
that maps input/hidden/output network variables $\nameVar$ of type $\tensorTypeVar$ to mathematical tensors of the corresponding type.

Given a list of network declarations $\networkDeclSet$, a list of models $\networkImplementationSet$ and an input assignment $\networkInputSet$, the environment $\assertEnv (\networkDeclSet, \networkImplementationSet, \networkInputSet)$ is calculated as the union of the environments local to each network:
\begin{equation*}
\assertEnv (\networkDeclSet, \networkImplementationSet, \networkInputSet) = \bigcup_i \assertEnv (\networkDeclSet_i, \networkImplementationSet_i, \networkInputSet_i)
\end{equation*}
Given a network declaration~$\networkDeclVar$, and a well-typed model~$\networkImplementation$ and an input assignment~$\networkInput$, the calculation of the local environment can be defined as follows:
\begin{align*}
\assertEnv (\networkExpansion, \networkImplementation, \networkInput) 
& = \assertEnv_{I} \cup \assertEnv_{H} \cup \assertEnv_{O}
\end{align*}
where
\newcommand{\nameFn}[1]{\text{name}(#1)}
\begin{align*}
    \assertEnv_{I} 
    &= \{ \nameFn{\inputDeclSet_i} \rightarrow \semTensorTheory{\networkInput_i} \}
    \\
    \assertEnv_{H} 
    &= \{ \nameFn{\hiddenDeclSet_i} \rightarrow {\color{red} ???} \}
    \\  
    \assertEnv_{O} 
    &= \{ \nameFn{\outputDeclSet_i} \rightarrow \semModelTheory{\networkImplementation}(\semTensorTheory{\networkInput_0}, \ldots, \semTensorTheory{\networkInput_n})_i \}
\end{align*}

\subsection{Assertions}

Given an environment $\assertEnv$ calculated as described in Section~\ref{sec:semantic-context}, we can calculate the semantics of assertions as: 
\input{diagrams/semantics-assertion.tex}
There is nothing particularly surprising about the semantics of boolean expressions to boolean values:
\input{diagrams/semantics-boolExpr.tex}
The only thing to note is that the exact semantics of comparisons is delegated to that of the underlying network theory $\networkTheoryVar$ to ensure that the correct behaviour for e.g. floating point comparisons.

The semantics of arithmetic expressions is defined as follows:
\input{diagrams/semantics-arithExpr.tex}
where $\sem{\prod}\networkTheoryVarParam$ and $\sem{\sum}\networkTheoryVarParam$ are defined in the obvious way in terms of $\sem{+}\networkTheoryVarParam$ and $\sem{\times}\networkTheoryVarParam$. As with comparisons, the actual mathematical operations are delegated to the underlying network theory $\networkTheoryVar$. Constants are interpreted as zero-dimensional tensors and the values of variables are looked up in the environment $\assertEnv$ and then indexed into appropriately.


\subsection{Queries}

As expected, the semantics of a query is defined in terms of the satisfiability problem it represents, i.e. given a set of networks, does there exist an assignment of input tensors that causes all the assertions to evaluate to true?
A query is said to be \textit{satisfiable} if there is such an input assignment, and \textit{unsatisfiable} otherwise.

More formally, the semantics of a query is defined as a function. The input to the function is a well-typed query $\queryVar$ and a syntactic list of models $\networkImplementationSet$ from the underlying theory $\networkTheoryVar$ that are well-typed with respect to $\queryVar$. The output of the function is the truth value of whether there exists a input assignment consisting of a syntactic list of tensors $\networkInputSet$ from the underlying theory that satisfies all the assertions concurrently:
\begin{equation*}
\begin{array}{lll}
\semQuery{(\queryExpansion),\networkImplementationSet} =
    \exists~(\networkInputSet : [\mgrammar{<tensor>}\networkTheoryVarParam]). \; \bigwedge^{m}_{i=1}~ 
    \semApp{assert}{A_i}^{\assertEnv(\networkImplementationSet,\networkInputSet)} \\
\end{array}
\end{equation*}
This is therefore exactly the question that a neural network solver is attempting to answer.

\subsection{Applications}

There are various applications of the semantics of the language, including:
\begin{itemize}
\item \textbf{proving the soundness of query transformations}, i.e. various transformations of a query preserves the meaning, e.g. conversion of assertions into Disjunctive Normal Form (DNF) do not change the satisfiability of the query. 
\item \textbf{proving the soundness of verifiers}, i.e. that a particular solver will give always give the right answer.
\item \textbf{proving the soundness of higher-level tools}, i.e. that tools that compile higher-level specifications down to a set of \vnnlib{} queries preserve the meaning of the original specification.
\end{itemize}


\section{Queries over \real}
\label{sec:real-queries}

It is well-known in the community that some solvers assume that the ONNX models operate over the real numbers $\mathbb{R}$ and therefore these solvers are not sound with respect to the semantics of the floating point types provided by the underlying network theory~$\networkTheoryVar$~\cite{jia2021exploiting}. 

Nonetheless, it is still desirable that \vnnlib{} supports such solvers. Therefore the \vnnlib{} query language also supports what is known as \emph{real-valued queries}. An example of such a query is shown in Figure~\ref{fig:real-simple-query}. Instead of using the element types provided by the underlying theory $\networkTheoryVar$, real-valued queries must exclusively use the special \inlinevnn{real} element type. 
Using the \inlinevnn{real} type gives the solver explicit permission to interpret the ONNX network files as \real{}-valued functions. The user is therefore explicitly acknowledging that the result may be unsound due to floating point imprecision. 

\begin{figure}[t]
    \begin{minipage}[c]{0.62\textwidth}
        \begin{lstlisting}[style=lbnf]
(declare-network myNetwork
    (declare-input  X real [1,10])
    (declare-output Y real [1,2])
)

(assert (>= X[0,2] 0.0))
(assert (<= X[0,2] 1.0))
(assert (<= Y[0,1] 0.5))\end{lstlisting}
    \end{minipage}%
    \begin{minipage}[c]{0.35\textwidth}
        \centering
        \includegraphics[height=5cm]{imgs/simple_net.onnx.pdf}
    \end{minipage}
    \caption{An example of a \vnnlib{} query over the real numbers, which explicitly gives the solver permission to (unsoundly) interpret the models from the network theory~$\networkTheoryVar$ as functions over \real. In reality, the network may operate over any floating point type.}
    \label{fig:real-simple-query}
\end{figure}

\begin{figure}[t]
\begin{grammar}
<elementType> \hspace{-0.25em}$^\real$ = \texttt{real}

<tensor> \hspace{-0.25em}$^\real$ = $(\shapeVar, \real^\shapeVar)$

<model> \hspace{-0.25em}$^\real$ = <model> \hspace{-0.25em}$\networkTheoryVarParam$
\end{grammar}

\begin{minipage}{0.6\textwidth}
\begin{equation*}
	\inferrule*[Right=(Element$^\real$)]
	{
        r \in \real
	}
	{
        \vdash r : \mathtt{real}
    }
    \hspace{8em}
    \inferrule*[Right=(Tensor$^\real$)]
    {
    }
    {
        \vdash (\shapeVar, \tensorVar) : (\elementTypeVar, \shapeVar)
    }
\end{equation*}
\begin{equation*}
	\inferrule*[Right=(Model$^\real$)]
	{
        \vdash\networkTheoryVarParam \modelVar : \modelTypeVar' \\
        \text{shapesIn}(\modelTypeVar') = \text{shapesIn}(\modelTypeVar)
    }
    {
        \vdash \modelVar : \modelTypeVar 
    }
\end{equation*}
\end{minipage}

\begin{minipage}{0.5\textwidth}
\begin{equation*}
\begin{array}{ll}
\semElementType{\mathtt{real}}^\real
&= \real
\\
\semTensor{\mathtt{real}, s}^\real
&= \real^s
\\
\semModel{\networkImplementation}^\real
&= \semApp{realModel}{\networkImplementation}\networkTheoryVarParam
\end{array}
\end{equation*}
\end{minipage}

\caption{Construction of a $\networkTheoryVar_\real$ from an abstract network theory $\networkTheoryVar$ and an additional function $\semApp{realModel}{\cdot}\networkTheoryVarParam$ that returns the semantics of a network model as a function over real numbers.}
\label{fig:real-network-theory}
\end{figure}

The issue is how to define the syntax and semantics of a real-valued query.
Luckily, the notion of a network theory gives us a modular way to do so. In particular, given a network theory $\networkTheoryVar$ and a function $\sem{\mathtt{realModel}}\networkTheoryVarParam$ that interprets models in $\networkTheoryVar$ as real-valued functions, we can define a new network theory $\networkTheoryVar_\real$ as shown in \autoref{fig:real-network-theory}. In particular:
\begin{itemize}
\item \textbf{Element types} in $\networkTheoryVar_\real$, consist of a single type, namely \texttt{real}, that has the expected typing rules and is interpreted in the semantics as $\real$.
\item \textbf{Tensors} in $\networkTheoryVar_\real$ are syntactically just mathematical tensors, again with the expected typing rules and semantics.
\item \textbf{Models} - models in $\networkTheoryVar_\real$ are models from the original theory $\networkTheoryVar$. They are well-typed as long as all the input and output tensors are of the same shape, i.e. the actual original element types provided by $\networkTheoryVar$ are irrelevant. The semantics of the model is provided by the function $\sem{\mathtt{realModel}}\networkTheoryVarParam$ that we assume we have access to.
\end{itemize}
We can then use this new theory $\networkTheoryVar_\real$, to obtain both the syntax and semantics for queries over $\real$ simply by reparametrising the exsting syntax and semantics defined in the previous sections. \autoref{fig:real-simple-query} shows an example query over $\networkTheoryVar_\real$.

\section{Mechanisation}

A fully formal version of the contents of this chapter, including the syntax, typing rules and semantics has been mechanised in the Agda interactive theorem prover.
This is available via the official \href{https://github.com/VNNLIB/VNNLIB-Agda}{Agda-VNNLIB} repository~\cite{vnnlib-agda}. 