% Define some colors for listings
\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}
\definecolor{keywordblue}{rgb}{0.13,0.13,1}
\definecolor{stringred}{rgb}{0.8,0,0}

% Listings style for Bash/CLI commands
\lstdefinestyle{lbnf}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{keywordblue}\bfseries,
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{stringred},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=1,
    frame=lines,
    xleftmargin=2em,
    framexleftmargin=1.5em,
    escapeinside={\%*}{*} 
}


The syntax of VNN-LIB is formally defined using Labelled Backus-Naur Form (LBNF)\cite{8}. LBNF is a variant of BNF that allows for 
annotations (labels) on productions, facilitating the automatic generation of abstract syntax trees, parsers, and other language processing tools. 
This formal grammar provides a rigorous foundation for the language, eliminating ambiguities present in previous versions and ensuring consistent 
parsing across different tools.

The full LBNF grammar for VNN-LIB is provided in the Appendix. The following subsections highlight key syntactic constructs of the language,
with examples illustrating their usage.

\subsection{Network Declaration}
VNN-LIB supports the definition of one or more neural networks within a single specification file. This is crucial for properties that need to refer to
multiple networks. A network declaration is introduced by the keyword \texttt{declare-network}, followed by a user-defined variable name for the network, 
and then its associated input, hidden (hidden), and output variable declarations. All variables are declared inside of network declarations and variable 
names must be unique within the scope of the entire VNN-LIB specification.

\begin{lstlisting}[
    caption=Network Definition Example, 
    style=lbnf,
    label={lst:network-definition}
]
(declare-network acc
    (declare-input X Real 3)
    (declare-hidden N Real 1 2 onnx-node:"node_name_in_onnx")
    (declare-output Y Real 3)
) 
\end{lstlisting}

For instance, Listing~\ref{lst:network-definition} declares a network named \texttt{acc}, with an input tensor \texttt{X}, hidden tensor \texttt{N}, 
and output tensor \texttt{Y}. 

\subsection{Input and Output Variable Declarations}

An input variable is declared using the \texttt{declare-input} keyword, followed by a variable name, its element type (e.g., \texttt{Real}, \texttt{int8}), 
and a space-seperated list of integers representing the shape of the tensor. Similarly, an output variable uses the \texttt{declare-output} keyword. Multiple 
input and output variables can be declared within a single network declaration, and the order of declaration determines the mapping to the nodes in the ONNX model.
The variables may be explicitly associated with ONNX node names using the \texttt{onnx-node} keyword and a string identifer, only under the condition that all input 
and output variables are associated with an ONNX node name.

\begin{lstlisting}[
  caption=Input and Output Definition Example,
  style=lbnf,
  label={lst:input-output}
]
(declare-input X Real 1 28 28) 
\end{lstlisting}

For example, Listing~\ref{lst:input-output} declares an input tensor named \texttt{X} of real numbers with shape $1 \times 28 \times 28$. 

\subsection{Hidden Node Declarations}

A hidden node is declared using the \texttt{declare-hidden} keyword. This declaration includes a variable name for use within the VNN-LIB specification, 
its element type, its tensor shape, and crucially, a string identifier that specifies the corresponding node name in the ONNX graph. The ability to declare hidden nodes
allows for properties to reference key intermediate computations within the network, such as encoding features, attention mechanisms, or other internal states. Multiple
hidden nodes can be trivially declared within a single network declaration.

\begin{lstlisting}[
    caption=hidden Node Declaration Example, 
    style=lbnf,
    label={lst:hidden-node}
]
(declare-hidden H1 Real 100 onnx-node:"layer3/relu_out") 
\end{lstlisting}

Listing~\ref{lst:hidden-node} declares an hidden variable \texttt{H1} of type \texttt{Real} with a shape of `(100)`. The \texttt{onnx-node} attribute 
specifies that this variable corresponds to the tensor named \texttt{"layer3/relu\_out"} in the associated ONNX graph. 

\subsection{Assertion Specification}
VNN-LIB supports quantifier-free logic formulas. Asertions are defined using parenthesised \texttt{(assert\ldots)} expressions, and follows an SMT-LIB-like syntax with the 
operand preceding its arguments. The operands can be logical operators (e.g., \texttt{and}, \texttt{or}) or arithmetic operators (e.g., \texttt{+}, \texttt{-}, \texttt{*}).
An assertion is a logical formula that consists of logical and arithmetic operations over one or more elements of the declared tensors.

\paragraph{Matrix Notation}
Let $X \in I$ be an $n$-dimensional tensor in some generic input domain $I = I^{d_1 \times \cdots \times d_n}$. The ``matrix notation'' represents a specific 
element $x_{i_1, i_2, \dots, i_n}$ of the tensor $X$ as \texttt{X\_$i_1$-$i_2$-\dots-$i_n$}, where $i_1, \dots, i_n$ are the indices of the element of interest in the 
dimensions $d_1, \dots, d_n$. To better clarify, if we consider the 1-D tensor $X \in I^n$, the 2-D tensor $Y \in I^{n \times m}$, and the 3-D tensor 
$Z \in I^{n \times m \times p}$, we will have the following representations:
\begin{itemize}
    \item \texttt{X\_0}, \texttt{X\_1}, \dots, \texttt{X\_$i$}, \dots, \texttt{X\_$n$};
    \item \texttt{Y\_0-0}, \texttt{Y\_0-1}, \dots, \texttt{Y\_$i$-$j$}, \dots, \texttt{Y\_$n$-$m$};
    \item \texttt{Z\_0-0-0}, \texttt{Z\_0-0-1}, \dots, \texttt{Z\_$i$-$j$-$k$}, \dots, \texttt{Z\_$n$-$m$-$p$};
\end{itemize}
In such a representation, \texttt{Z\_$i$-$j$-$k$} corresponds to the element $z_{i,j,k}$ of the tensor $Z$. 

\paragraph{Assertion Example}
For example, Listing~\ref{lst:assertion-example} asserts that for a given range of the input neuron $A_1$, the output neuron $B_0$ 
is greater than another output neuron $B_1$. More complex properties, including those referencing multiple networks or hidden nodes, 
can be constructed using these foundational elements.

\begin{lstlisting} [
	caption=Assertion Example, 
	style=lbnf,
    label={lst:assertion-example}
]
(assert 
    (and 
        (and (>= A_0 0.0) (<= B_0 1.0)) 
        (> B_0 B_1)
    )
)
\end{lstlisting}
